{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import inspect\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.transforms import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "from torchvision.datasets import VOCDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from TRUST_preprocessing import split_n_rotate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{torch.cuda.current_device()}') if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TRUSTConfig:\n",
    "    N: int = 100\n",
    "    M: int = 100\n",
    "    d: int = 8\n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 100\n",
    "    num_decoders: int = 6\n",
    "    n_heads: int = 4\n",
    "    block_size: int = 8\n",
    "    bias: int = False\n",
    "    \n",
    "\n",
    "\n",
    "config = TRUSTConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resnet_with_fpn.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*list(resnet18().children())[:8])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRUSTDataset(Dataset):\n",
    "    def __init__(self, config, root, anno_dir, img_dir):\n",
    "        super().__init__()\n",
    "        self.N = config.N\n",
    "        self.M = config.M\n",
    "        self.anno_dir = root+anno_dir\n",
    "        self.img_dir = root+img_dir\n",
    "        self.img_list = []\n",
    "        with open(self.anno_dir) as f:\n",
    "            self.anno_list = json.load(f)\n",
    "        for img_anno in self.anno_list:\n",
    "            self.img_list.append(cv2.imread(img_anno['filename']))  \n",
    "        self.angles = [np.random.choice(np.arange(-45, 46), size=1000) for _ in range(len(self.anno_list))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anno_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_anno = self.anno_list[index]\n",
    "        img = self.img_list[index]\n",
    "        # angle_prob = self.angle_probs[index]\n",
    "        # normalized_prob = np.linalg.norm(angle_prob)\n",
    "        # angle = np.random.choice()\n",
    "\n",
    "        angle = np.random.randint(-45, 46)\n",
    "        rotated_img, trust_anno = split_n_rotate(img, img_anno['col_separators'], img_anno['row_separators'], self.N, self.M, angle)\n",
    "\n",
    "        return rotated_img, trust_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_with_FPN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d = config.d\n",
    "        self.resnet = resnet18()\n",
    "        self.pyramid = []\n",
    "        self.return_nodes = {\n",
    "            'layer1.1.conv2': 'layer1', #P2\n",
    "            'layer2.1.conv2': 'layer2', #P3\n",
    "            'layer3.1.conv2': 'layer3', #P4\n",
    "            'layer4.1.conv2': 'layer4'  #P5\n",
    "        }\n",
    "        self.feature_extractor = create_feature_extractor(self.resnet, return_nodes=self.return_nodes)\n",
    "        self.conv1_list = nn.ModuleList([\n",
    "            nn.Conv2d(64, 1, (1,1), 1),\n",
    "            nn.Conv2d(128, 1, (1,1), 1),\n",
    "            nn.Conv2d(256, 1, (1,1), 1),\n",
    "            nn.Conv2d(512, 1, (1,1), 1),\n",
    "        ])\n",
    "        self.conv3_list = nn.ModuleList([nn.Conv2d(1, self.d, (3,3), 2) for _ in range(4)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        intermediate_outputs = self.feature_extractor(x)\n",
    "        pyramid = []\n",
    "        outs = []\n",
    "        for i in range(3, -1, -1):\n",
    "            layer_name = 'layer'+str(i)\n",
    "            conv1 = self.conv1_list[i]\n",
    "            conv3 = self.conv3_list[i]\n",
    "            pyramid[i] = conv1(intermediate_outputs[layer_name])\n",
    "            if i<3:\n",
    "                pyramid[i] += F.interpolate(pyramid[i+1], scale_factor=(2,2))\n",
    "            outs[i] = conv3(pyramid[i])\n",
    "        return outs\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"TRUST.jpg\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[[1,2,3],[1,2,3]]])\n",
    "b = a.unsqueeze(2).repeat(1,1,5,1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.d\n",
    "        self.n_heads = config.n_heads\n",
    "        self.block_size = config.block_size\n",
    "        self.c_attn_q = nn.Linear(self.n_embd, self.n_embd, bias=config.bias)\n",
    "        self.c_attn_kv = nn.Linear(self.n_embd, self.n_embd*2, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=config.bias)\n",
    "        # self.mask = torch.tril(torch.ones((self.block_size, self.block_size))).view(1, 1, self.block_size, self.block_size)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, pos_encoding, encoder_output):\n",
    "        assert x.shape[0] == encoder_output.shape[0]\n",
    "        B, T, C = encoder_output.shape #batch_size, block_size, n_embd\n",
    "        B, N, C = x.shape #batch_size, n_queries, n_embd\n",
    "        print(f\"T:{T} N:{N}\")\n",
    "        k0, v = self.c_attn_kv(encoder_output).split(self.n_embd, dim=2)\n",
    "        q0 = self.c_attn_q(x)\n",
    "        k = k0 + pos_encoding\n",
    "        q = q0\n",
    "\n",
    "        q = q.view(B, N, self.n_heads, C//self.n_heads).transpose(1,2) # (B, n_heads, N, h_size)\n",
    "        k = k.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2) # (B, n_heads, T, h_size)\n",
    "        v = v.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2)\n",
    "\n",
    "        att = q@k.transpose(-2,-1)*(1/math.sqrt(k.size(-1))) # (B, n_heads, N, T)\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att@v\n",
    "        y = y.transpose(1,2).contiguous().view(B,N,C) #(B, n_heads, N, h_size)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config, masked=False):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.d\n",
    "        self.n_heads = config.n_heads\n",
    "        self.block_size = config.block_size\n",
    "        self.c_attn = nn.Linear(self.n_embd, self.n_embd*3, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=config.bias)\n",
    "        self.mask = torch.tril(torch.ones((self.block_size, self.block_size))).view(1, 1, self.block_size, self.block_size)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.masked = masked\n",
    "        \n",
    "\n",
    "    def forward(self, x, pos_encoding):\n",
    "        B, T, C = x.shape #batch_size, n_embd, block_size\n",
    "        q0, k0, v = self.c_attn(x).split(self.n_embd, dim=2) \n",
    "        q = q0 + pos_encoding\n",
    "        k = k0 + pos_encoding\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2) # (B, n_heads, T, h_size)\n",
    "        k = k.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2) \n",
    "        v = v.view(B, T, self.n_heads, C//self.n_heads).transpose(1,2)\n",
    "\n",
    "        att = q@k.transpose(-2,-1)*(1/math.sqrt(k.size(-1))) # (B, n_heads, T, T)\n",
    "        if self.masked:\n",
    "            att = att.masked_fill_(self.mask[:, :, :T, :T] == 0, -float('inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att@v\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C) #(B, n_heads, T, h_size)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "#query-based splitting module\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d = config.d\n",
    "        self.self_attention = SelfAttention(config, masked=True)\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.cross_attention = CrossAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "        self.ln_3 = nn.LayerNorm(config.n_embd)\n",
    "    def forward(self, prev_output, pos_encoding, encoder_output):\n",
    "        x = self.self_attention(prev_output, pos_encoding)\n",
    "        x = self.ln_1(x)\n",
    "        x += self.cross_attention(x, pos_encoding, encoder_output)\n",
    "        x = self.ln_2(x)\n",
    "        x += self.mlp(x)\n",
    "        x = self.ln_3(x)\n",
    "        return x\n",
    "# class Decoder(nn.Module): #transformer decoder\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.d = config.d\n",
    "#     def forward(self, x):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TRUST(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        self.d = config.d\n",
    "        self.N = config.N\n",
    "        self.M = config.M\n",
    "        self.num_decoders = config.num_decoders\n",
    "        self.bias = config.bias\n",
    "        self.visual_features = ResNet_with_FPN(config)\n",
    "        self.row_embedding = nn.Embedding(self.N, self.d)\n",
    "        self.col_embedding = nn.Embedding(self.M, self.d)\n",
    "        self.row_decoder = nn.ModuleList([DecoderBlock(config) for _ in range(self.num_decoders)])\n",
    "        self.col_decoder = nn.ModuleList([DecoderBlock(config) for _ in range(self.num_decoders)])\n",
    "        self.row_separator_decoder = nn.ModuleList([DecoderBlock(config) for _ in range(self.num_decoders)])\n",
    "        self.col_separator_decoder = nn.ModuleList([DecoderBlock(config) for _ in range(self.num_decoders)])\n",
    "        self.row_fc = nn.Linear(self.d, 3, bias=self.bias)\n",
    "        self.col_fc = nn.Linear(self.d, 3, bias=self.bias)\n",
    "        self.vertex_fc = nn.Linear(self.d, 4, bias=self.bias)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        B, _, _, _ = x.shape\n",
    "        x = self.visual_features(x)\n",
    "        P2_unflattened = x[0]\n",
    "        P2 = P2_unflattened.flatten(start_dim=2)\n",
    "        P2 = P2.transpose(1,2)\n",
    "        row_queries = self.row_embedding(torch.arange(self.N).repeat(B,1))\n",
    "        row_features = row_queries\n",
    "        col_queries = self.col_embedding(torch.arange(self.M).repeat(B,1))\n",
    "        col_features = col_queries\n",
    "        for block in self.row_decoder:\n",
    "            row_features = block(row_features, row_queries, P2) #B, N, d\n",
    "        for block in self.col_decoder:\n",
    "            col_features = block(col_features, col_queries, P2) #B, M, d\n",
    "        row_separators = self.row_fc(row_features)\n",
    "        enhanced_row_separators = row_separators\n",
    "        col_separators = self.col_fc(col_features)\n",
    "        enhanced_col_separators = col_separators\n",
    "        for block in self.row_separator_decoder:\n",
    "            enhanced_row_separators = block(enhanced_row_separators, row_separators, col_separators)\n",
    "        for block in self.col_separator_decoder:\n",
    "            enhanced_col_separators = block(enhanced_col_separators, col_separators, row_separators)\n",
    "        enhanced_row_separators = enhanced_row_separators.unsqueeze(2).repeat(1,1,self.M,1)\n",
    "        enhanced_col_separators = enhanced_col_separators.unsqueeze(1).repeat(1,self.N,1,1)\n",
    "        vertex_features = enhanced_row_separators + enhanced_col_separators\n",
    "        merge_features = self.vertex_fc(vertex_features)\n",
    "        row_cls_criterion = nn.BCELoss()\n",
    "        col_cls_criterion = nn.BCELoss()\n",
    "        link_cls_criterion = nn.BCELoss()\n",
    "        angle_cls_criterion = nn.CrossEntropyLoss()\n",
    "        start_point_criterion = nn.SmoothL1Loss(beta=1)\n",
    "\n",
    "        if target is not None:\n",
    "            #online hard example mining\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
